{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0231297b-8db8-45fd-a7d9-200e8cba8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import albumentations as albu\n",
    "import albumentations.pytorch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import timm \n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from pprint import pprint\n",
    "#!pip install facenet-pytorch\n",
    "#!conda install -c conda-forge ipywidgets\n",
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e8826d-b45e-4d10-a776-7ec076ff7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# with open('images_path.pickle','rb') as f:\n",
    "#     images_path = pickle.load(f)\n",
    "    \n",
    "batch_size = 256\n",
    "MODEL_PATH =\"saved\"\n",
    "folder_path = '/opt/ml/input/data/train/images/'\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "val_rate = 0.1\n",
    "logs_base_dir = \"logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "exp  = f\"{logs_base_dir}/AgeSmooth\"\n",
    "writer = SummaryWriter(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbfef16a-d308-4de8-837b-1878b33eb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self,folder_path ,image_path ,label,transform=None, train=True):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_path = image_path\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.info = pd.read_csv(self.image_path)\n",
    "        self.y = self.info[self.label]\n",
    "        self.x = self.info['path']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        y = None\n",
    "        if self.label == 'age':\n",
    "            if self.y[index] < 30:\n",
    "                y = torch.tensor([1,0,0])\n",
    "            elif 30 <= self.y[index] <= 50:\n",
    "                y = torch.tensor([0,1,0])\n",
    "            elif 50 < self.y[index] < 60:\n",
    "                y = torch.tensor([0,1,1])\n",
    "            else:\n",
    "                y = torch.tensor([0,0,1])\n",
    "                \n",
    "        elif self.label == 'gender':\n",
    "            if self.y[index] == 'male':\n",
    "                y = torch.tensor(0)\n",
    "            else:\n",
    "                y = torch.tensor(1)\n",
    "        elif self.label == 'state':\n",
    "            if self.y[index] == 'mask':\n",
    "                y = torch.tensor(0)\n",
    "            elif self.y[index] == 'incorrect':\n",
    "                y = torch.tensor(1)\n",
    "            elif self.y[index] == 'normal':\n",
    "                y = torch.tensor(2)\n",
    "        x = np.array(Image.open(folder_path+self.x[index]))\n",
    "        if self.transform:\n",
    "            x = self.transform(image=x)\n",
    "        return x['image'], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081f6f22-afca-4d79-87a8-829ec276345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = 0\n",
    "        correct += torch.sum(pred == target).item()\n",
    "    return correct / len(target)\n",
    "\n",
    "def accuracy_multi_label(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        origin_target = torch.argmax(target, dim=1)\n",
    "        # [0, 1, 1] -> [0, 1, 0] -> 2\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = 0\n",
    "        correct += torch.sum(pred == origin_target).item()\n",
    "    return correct / len(target)\n",
    "\n",
    "def f1(output, target, is_training=False):\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "\n",
    "    assert pred.ndim == 1\n",
    "    assert target.ndim == 1 or target.ndim == 2\n",
    "\n",
    "    if target.ndim == 2:\n",
    "        target = target.argmax(dim=1)\n",
    "\n",
    "    tp = (target * pred).sum().to(torch.float32)\n",
    "    tn = ((1 - target) * (1 - pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - target) * pred).sum().to(torch.float32)\n",
    "    fn = (target * (1 - pred)).sum().to(torch.float32)\n",
    "\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1\n",
    "\n",
    "def f1_multi_label(output, target, is_training=False):\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    \n",
    "    origin_target = torch.argmax(target, dim=1)\n",
    "\n",
    "    assert pred.ndim == 1\n",
    "    assert target.ndim == 1 or target.ndim == 2\n",
    "\n",
    "    if origin_target.ndim == 2:\n",
    "        origin_target = origin_target.argmax(dim=1)\n",
    "\n",
    "    tp = (origin_target * pred).sum().to(torch.float32)\n",
    "    tn = ((1 - origin_target) * (1 - pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - origin_target) * pred).sum().to(torch.float32)\n",
    "    fn = (origin_target * (1 - pred)).sum().to(torch.float32)\n",
    "\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20380aa-9ad6-49e0-9b01-38b2cdf40435",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug = albu.Compose([albu.ColorJitter(brightness=(0.2, 1), contrast=(0.3, 1), saturation=(0.2, 1), hue=(-0.3, 0.3)),\n",
    "            albu.RandomCrop(300, 300),\n",
    "            albu.HorizontalFlip(),\n",
    "            albu.Resize(224, 224),\n",
    "            albu.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "            albu.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "vaild_preprocess = albu.Compose([albu.Resize(224, 224),\n",
    "                                 albu.Normalize(mean=(0.5, 0.5, 0.5),std=(0.2, 0.2, 0.2)),\n",
    "                                 albu.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "train_age_data = CustomDataSet(folder_path,'train_path.csv', 'age', transform = train_aug, train=True)\n",
    "train_age_data_loader = DataLoader(dataset=train_age_data,batch_size=batch_size, shuffle=True,drop_last=True,num_workers=2)\n",
    "vaild_age_data = CustomDataSet(folder_path,'train_path.csv', 'age', transform = vaild_preprocess, train=False)\n",
    "vaild_age_data_loader = DataLoader(dataset=vaild_age_data,batch_size=batch_size, shuffle=False,drop_last=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2d8c5e-c9f5-48f3-9cde-690ac73cd5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeSmoothClassifier(nn.Module):\n",
    "    def __init__(self, num_of_classes = 3):\n",
    "        super().__init__()\n",
    "        self.m = InceptionResnetV1(classify=True,num_classes=3)\n",
    "    def forward(self, x):\n",
    "        x = self.m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9acc54f6-6092-456b-91c2-2b62c505e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_clf = AgeSmoothClassifier()\n",
    "age_clf = age_clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bebab01-ad71-43e5-a2cf-90d255010598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.17836 | Acc: 0.820 | VAcc: 0.734 | F1: 0.788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b3b13ce00794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvaild_age_data_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mage_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.5,1.0]).to(device)) #1.5,1.0\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = optim.Adam(age_clf.parameters(), lr=LEARNING_RATE,amsgrad=True)\n",
    "\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_age_data_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = age_clf(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = accuracy_multi_label(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += acc\n",
    "    writer.add_scalar('Loss/train', loss, e)\n",
    "    writer.add_scalar('Acc/train', acc, e)\n",
    "    \n",
    "    age_clf.eval()\n",
    "    vaild_acc = 0\n",
    "    f1 = 0\n",
    "    for data, labels, in vaild_age_data_loader:\n",
    "        with torch.no_grad():\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "            y_pred = age_clf(data)\n",
    "            vacc = accuracy_multi_label(y_pred, labels)\n",
    "            vaild_acc += vacc\n",
    "            f1 += f1_multi_label(y_pred,labels)\n",
    "    writer.add_scalar('F1/val', f1, e)\n",
    "    age_clf.train()\n",
    "    va = f'{vaild_acc/len(vaild_age_data_loader):.3f}' \n",
    "    l = f'{epoch_loss/len(train_age_data_loader):.5f}'\n",
    "    a = f'{epoch_acc/len(train_age_data_loader):.3f}'\n",
    "    f1 = f'{f1/len(vaild_age_data_loader):.3f}'\n",
    "    print(f'Epoch {e+0:03}: | Loss: {l} | Acc: {a} | VAcc: {va} | F1: {f1}')\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    torch.save(age_clf.state_dict(), os.path.join(MODEL_PATH, f'age_smooth_model_{e+0:03}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8b8d1-ce71-4e38-8a4f-2c519ce243e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images, labels =next(iter(train_age_data_loader))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "# for n, (image, label) in enumerate(zip(images, labels), start=1):\n",
    "#     plt.subplot(4,4,n)\n",
    "#     plt.imshow(transforms.ToPILImage()(image))  # Normalize 처리때문에 복구\n",
    "#     plt.title(\"{}\".format(label))\n",
    "#     plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()    \n",
    "\n",
    "# images, labels =next(iter(vaild_age_data_loader))\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "# for n, (image, label) in enumerate(zip(images, labels), start=1):\n",
    "#     plt.subplot(4,4,n)\n",
    "#     plt.imshow(transforms.ToPILImage()(image))  # Normalize 처리때문에 복구\n",
    "#     plt.title(\"{}\".format(label))\n",
    "#     plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
