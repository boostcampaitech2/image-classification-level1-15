{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b667437f-f631-46e5-b8b6-3352e4e18748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9a7e10-f10a-4b3d-b8ce-69e14a1f1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = 0\n",
    "        correct += torch.sum(pred == target).item()\n",
    "    return correct / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ce9a58-ce59-47eb-8ed5-71f15e81d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "with open('images_path.pickle','rb') as f:\n",
    "    images_path = pickle.load(f)\n",
    "    \n",
    "batch_size = 64\n",
    "MODEL_PATH =\"saved\"\n",
    "folder_path = '/opt/ml/input/data/train/images/'\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469e96de-71d7-439b-bf4e-91a9ccc215d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateClassifier(nn.Module):\n",
    "    def __init__(self, num_of_classes = 3):\n",
    "        super().__init__()\n",
    "        self.m = timm.create_model('efficientnet_b1',pretrained=True)\n",
    "        self.fc = nn.Linear(self.m.classifier.out_features, num_of_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.m(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e95d0a6-8284-47c2-9d02-507d10b75fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskStateDataSet(Dataset):\n",
    "    def __init__(self,f_path,images_path,transform=None,train=True):\n",
    "        self.f_path = f_path\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform\n",
    "        self.info = pd.read_csv('image_info.csv')\n",
    "        self.y = self.info['mask_state']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        y = torch.tensor(self.y[index])\n",
    "\n",
    "        x = Image.open(self.f_path + self.images_path[index])\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc328090-4ba6-47d2-a4a6-54067621272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_clf = StateClassifier()\n",
    "mask_state_data = MaskStateDataSet(f_path=folder_path,\n",
    "                                        images_path=images_path,\n",
    "                                        transform=transforms.Compose([Resize((260,260)),ToTensor(),\n",
    "                                        Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))]))\n",
    "\n",
    "mask_state_data_loader = DataLoader(dataset=mask_state_data,batch_size=batch_size,\n",
    "                                        shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090a09ea-e238-4256-9d4d-c0a9ba761d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.05483 | Acc: 0.982\n",
      "Epoch 002: | Loss: 0.02274 | Acc: 0.994\n",
      "Epoch 003: | Loss: 0.00622 | Acc: 0.998\n",
      "Epoch 004: | Loss: 0.00366 | Acc: 0.999\n",
      "Epoch 005: | Loss: 0.00192 | Acc: 0.999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a11ffd34439c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "state_clf = state_clf.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.,1.,6.]).to(device))\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "optimizer = optim.Adam(state_clf.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in mask_state_data_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()        \n",
    "        y_pred = state_clf(X_batch)\n",
    "               \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = accuracy(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "\n",
    "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(mask_state_data_loader):.5f} | Acc: {epoch_acc/len(mask_state_data_loader):.3f}')\n",
    "    l = f'{epoch_loss/len(mask_state_data_loader):.5f}'\n",
    "    a = f'{epoch_acc/len(mask_state_data_loader):.3f}'\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    torch.save(state_clf.state_dict(), os.path.join(MODEL_PATH, f'state_model_{l}_{a}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7311dda-7d7e-464b-ba80-d7d3bf8c8eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
