{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a7a054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98c4d431-c88e-4612-a470-e05cd6c2f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = 0\n",
    "        correct += torch.sum(pred == target).item()\n",
    "    return correct / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d94c0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "435fe503",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('images_path.pickle','rb') as f:\n",
    "    images_path = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0891f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/opt/ml/input/data/train/images/'\n",
    "MODEL_PATH =\"saved\"\n",
    "batch_size = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "val_rate = 0.1\n",
    "# im_path = folder_path + images_path[0]\n",
    "# #im = Image.open('/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask.jpg')\n",
    "# im = Image.open(im_path)\n",
    "# im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab7420ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskGenderDataSet(Dataset):\n",
    "    def __init__(self,f_path,images_path,transform=None,train=True):\n",
    "        self.f_path = f_path\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform\n",
    "        self.info = pd.read_csv('image_info.csv')\n",
    "        self.y = self.info['sex']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        y = self.y[index]\n",
    "        if y == 'male':\n",
    "            y = 0\n",
    "        else:\n",
    "            y = 1\n",
    "        x = Image.open(self.f_path + self.images_path[index])\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8d1381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_gender_data = MaskGenderDataSet(f_path=folder_path,\n",
    "                                        images_path=images_path,\n",
    "                                        transform=transforms.Compose([transforms.RandomCrop(300),\n",
    "                                                                      transforms.RandomPerspective(),\n",
    "                                                                      transforms.RandomPerspective(),\n",
    "                                                                      transforms.RandomHorizontalFlip(),\n",
    "                                                                      transforms.RandomGrayscale(),\n",
    "                                                                      Resize((260,260)),\n",
    "                                                                      ToTensor(),\n",
    "                                                                      Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "                                                                      ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92564289-9546-4d44-8606-2c5552d58096",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_gender_data, validation_data = torch.utils.data.random_split(mask_gender_data,[int(len(mask_gender_data)*(1-val_rate)),\n",
    "                                                                                    int(len(mask_gender_data)*val_rate)])\n",
    "\n",
    "mask_gender_data_loader = DataLoader(dataset=mask_gender_data,batch_size=batch_size,\n",
    "                                        shuffle=True,drop_last=True,num_workers=4)\n",
    "\n",
    "validation_loader = DataLoader(dataset = validation_data, batch_size = batch_size, drop_last=True,shuffle=True)\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b99cd76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images, labels =next(iter(validation_loader))\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "# for n, (image, label) in enumerate(zip(images, labels), start=1):\n",
    "#     plt.subplot(4,4,n)\n",
    "#     plt.imshow(transforms.ToPILImage()(image))  # Normalize 처리때문에 복구\n",
    "#     plt.title(\"{}\".format(label))\n",
    "#     plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18203833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images, labels =next(iter(mask_age_data_loader))\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "# for n, (image, label) in enumerate(zip(images, labels), start=1):\n",
    "#     plt.subplot(4,4,n)\n",
    "#     plt.imshow(transforms.ToPILImage()(image))  # Normalize 처리때문에 복구\n",
    "#     plt.title(\"{}\".format(label))\n",
    "#     plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6ec295d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# images, labels =next(iter(mask_state_data_loader))\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "# for n, (image, label) in enumerate(zip(images, labels), start=1):\n",
    "#     plt.subplot(4,4,n)\n",
    "#     plt.imshow(transforms.ToPILImage()(image))  # Normalize 처리때문에 복구\n",
    "#     plt.title(\"{}\".format(label))\n",
    "#     plt.axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f51fe425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderClassifier(nn.Module):\n",
    "    def __init__(self, num_of_classes = 2):\n",
    "        super().__init__()\n",
    "        self.m = timm.create_model('efficientnet_b1',pretrained=True)\n",
    "        self.fc = nn.Linear(self.m.classifier.out_features, num_of_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.m(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01b2b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = 0\n",
    "        correct += torch.sum(pred == target).item()\n",
    "    return correct / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b50f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_clf = GenderClassifier()\n",
    "\n",
    "# for parm in gender_clf.parameters():\n",
    "#     parm.requires_grad = False\n",
    "# for parm in gender_clf.fc.parameters():\n",
    "#     parm.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f7703e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Loss: 0.14454 | Acc: 0.942 | VAcc: 0.980\n",
      "Epoch 002: | Loss: 0.07320 | Acc: 0.973 | VAcc: 0.983\n",
      "Epoch 003: | Loss: 0.05096 | Acc: 0.980 | VAcc: 0.978\n",
      "Epoch 004: | Loss: 0.04540 | Acc: 0.983 | VAcc: 0.985\n",
      "Epoch 005: | Loss: 0.04186 | Acc: 0.985 | VAcc: 0.988\n",
      "Epoch 006: | Loss: 0.03795 | Acc: 0.986 | VAcc: 0.990\n",
      "Epoch 007: | Loss: 0.03576 | Acc: 0.988 | VAcc: 0.988\n",
      "Epoch 008: | Loss: 0.02905 | Acc: 0.990 | VAcc: 0.991\n",
      "Epoch 009: | Loss: 0.02725 | Acc: 0.990 | VAcc: 0.987\n",
      "Epoch 010: | Loss: 0.03216 | Acc: 0.989 | VAcc: 0.989\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-c06242e255e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gender_clf = gender_clf.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.5,1.0]).to(device))\n",
    "optimizer = optim.Adam(gender_clf.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in mask_gender_data_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = gender_clf(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        acc = accuracy(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() \n",
    "        epoch_acc += acc\n",
    "    \n",
    "    gender_clf.eval()\n",
    "    vaild_acc = 0\n",
    "    for data, labels, in validation_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        y_pred = gender_clf(data)\n",
    "        vacc = accuracy(y_pred, labels)\n",
    "        vaild_acc += vacc\n",
    "    gender_clf.train()\n",
    "    va = f'{vaild_acc/len(validation_loader):.3f}' \n",
    "    l = f'{epoch_loss/len(mask_gender_data_loader):.5f}'\n",
    "    a = f'{epoch_acc/len(mask_gender_data_loader):.3f}'\n",
    "    print(f'Epoch {e+0:03}: | Loss: {l} | Acc: {a} | VAcc: {va}')\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    torch.save(gender_clf.state_dict(), os.path.join(MODEL_PATH, f'gender_model_{e+0:03}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH =\"saved\"\n",
    "# if not os.path.exists(MODEL_PATH):\n",
    "#     os.makedirs(MODEL_PATH)\n",
    "# torch.save(gender_clf.state_dict(), os.path.join(MODEL_PATH, \"gender_model.pt\"))\n",
    "# '''\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# new_model = TheModelClass()\n",
    "# new_model.load_state_dict(torch.load(os.path.join(\n",
    "#     MODEL_PATH, \"model.pt\")))\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904b8f8-845e-4642-8970-2b676e8e9b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del mask_age_data_loader\n",
    "# del age_clf\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76700f69-5598-414c-b0c1-ff28122891b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50cfb1e-c487-4867-9dc0-f38c05c3fdef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
